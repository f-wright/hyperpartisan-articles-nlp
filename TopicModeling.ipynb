{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ab321b-cb2a-4b8b-b4a9-e90b91209bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: tomotopy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.12.7)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tomotopy) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18681b29-a470-44af-9ae1-2e04f21ba269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c02d67-0ced-4f68-8bdb-26a411175667",
   "metadata": {},
   "source": [
    "## Access XML data for topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb2ae85-8de6-49f6-ad43-b1ce4aeeb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0b1705-94a8-4124-8bfa-b9b82f47e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tree = et.parse(\"data/articles-training-byarticle-20181122.xml\")\n",
    "publisher_tree = et.parse(\"data/articles-training-bypublisher-20181122.xml\")\n",
    "\n",
    "article_root = article_tree.getroot()\n",
    "publisher_root = publisher_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5e6884-4fe2-4d49-94ad-36f8a17e70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fwright/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11686f0e-fb1f-4b7a-8339-db735ca32e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aren', 'about', 'all', 'by', 'against', 'such', \"aren't\", 'under', 'while', 'where', 'does', 'not', 's', 'hers', \"weren't\", 'are', 'before', 'don', 'what', 'has', 'on', 'own', 'your', 'each', \"hasn't\", 'after', 'further', 'during', 'here', \"haven't\", 'if', 'shan', \"you'd\", 'above', \"you're\", 'wouldn', 'until', 'wasn', 'for', 'yours', 'some', 'was', \"couldn't\", 'very', 'a', 'he', 'do', \"she's\", 'were', 'ourselves', 'no', 'then', 'shouldn', 'ours', 'but', 'their', 'or', \"don't\", 'that', 'to', 'up', 'than', 'll', 'those', 'whom', 'our', 'when', \"you'll\", 'why', 'will', 'been', 'at', 'theirs', 'did', 'who', 'just', 've', 'me', 'themselves', \"should've\", 'with', 'you', 'its', 'herself', 'again', 'ain', 'over', 'any', 'same', 'm', 'doesn', 'ma', 'they', \"wouldn't\", 'most', \"mustn't\", 'them', 'won', 're', 'hasn', 'mustn', 'into', 'how', 'it', 'both', 'few', 'weren', 'only', 'him', \"wasn't\", 'couldn', 'd', 'these', 'because', 'and', \"shouldn't\", 'can', 'so', 'having', \"needn't\", 'yourselves', 'yourself', 'himself', 'have', 'doing', 'itself', 'my', 'the', \"isn't\", 'am', 'should', \"mightn't\", 'from', 'is', 'in', 'myself', 'y', \"it's\", 'an', 'now', 'had', 'which', 'o', 'be', 'she', \"that'll\", \"hadn't\", 'too', \"won't\", 'through', 'didn', 'being', 'nor', \"didn't\", \"doesn't\", \"you've\", 'below', 'isn', 'his', 'this', 'between', 'more', 'hadn', 'of', 'haven', 'off', 'out', 'once', 'needn', 'i', \"shan't\", 'down', 'her', 'as', 'other', 'we', 't', 'there', 'mightn'}\n"
     ]
    }
   ],
   "source": [
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66769e5-48a0-4bd2-826f-f3c0f15db599",
   "metadata": {},
   "source": [
    "In the cell below, we get the text out of each of our articles to pass into the topic model. We apply lowercase to all of our words so that words with capital letters are treated the same as their lowercase counterparts, and so that words with capital letters will also be checked against the stoplist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91ba3109-49eb-4239-a653-ebb370fa3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n"
     ]
    }
   ],
   "source": [
    "article_list = []\n",
    "\n",
    "for article in article_root:\n",
    "    article_text = []\n",
    "    for paragraph in article:\n",
    "        if paragraph.text is not None:\n",
    "            article_text += [word for word in paragraph.text.lower().split() if word not in stoplist and word.isalpha()]\n",
    "    article_list += [article_text]\n",
    "print(len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75dbabbd-d150-4115-99a3-c1f1577d546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600645\n"
     ]
    }
   ],
   "source": [
    "for article in publisher_root:\n",
    "    article_text = []\n",
    "    for paragraph in article:\n",
    "        if paragraph.text is not None:\n",
    "            article_text += [word for word in paragraph.text.lower().split() if word not in stoplist and word.isalpha()]\n",
    "    article_list += [article_text]\n",
    "print(len(article_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45e54b-6acb-43fe-b573-ad5b8cbc2ed3",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d967be-1dda-426c-bb37-feb1573233fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_docs = len(article_list)\n",
    "\n",
    "mdl = tomotopy.LDAModel(k=n_topics)\n",
    "for article in article_list:\n",
    "  mdl.add_doc(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcba743a-1818-4fdb-acb8-6b237588fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50\tLog-likelihood: -9.205508485789876\n",
      "Iteration: 100\tLog-likelihood: -9.176238038682477\n",
      "Iteration: 150\tLog-likelihood: -9.166043252812388\n",
      "Iteration: 200\tLog-likelihood: -9.160990202818681\n",
      "Iteration: 250\tLog-likelihood: -9.151298462028565\n",
      "Iteration: 300\tLog-likelihood: -9.145877663961532\n",
      "Iteration: 350\tLog-likelihood: -9.142571228333345\n",
      "Iteration: 400\tLog-likelihood: -9.139582995243835\n",
      "Iteration: 450\tLog-likelihood: -9.137798502337274\n",
      "Iteration: 500\tLog-likelihood: -9.136645595088805\n",
      "Iteration: 550\tLog-likelihood: -9.135171346654491\n",
      "Iteration: 600\tLog-likelihood: -9.133254441994875\n",
      "Iteration: 650\tLog-likelihood: -9.13181960538584\n",
      "Iteration: 700\tLog-likelihood: -9.130851064191335\n",
      "Iteration: 750\tLog-likelihood: -9.13033692686749\n",
      "Iteration: 800\tLog-likelihood: -9.129721959549837\n",
      "Iteration: 850\tLog-likelihood: -9.129472683605906\n",
      "Iteration: 900\tLog-likelihood: -9.129412680815614\n",
      "Iteration: 950\tLog-likelihood: -9.12939344539318\n",
      "Iteration: 1000\tLog-likelihood: -9.129210351095743\n"
     ]
    }
   ],
   "source": [
    "iters_per_check = 50\n",
    "for i in range(0, 1000, iters_per_check):\n",
    "    mdl.train(iters_per_check)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i+iters_per_check, mdl.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5336d5af-f210-453e-bfb0-469cbe104d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words by topic\n",
      "#0: first game last two team points second said coach three\n",
      "#1: would tax health new percent million could money people pay\n",
      "#2: like one people get think know going time would really\n",
      "#3: us one people even would american political many war like\n",
      "#4: said police court two state told law according say one\n",
      "#5: said new water energy also oil could climate one would\n",
      "#6: company said percent stock market continue new last billion million\n",
      "#7: said military government united president would also security international north\n",
      "#8: trump president said would house obama republican white campaign new\n",
      "#9: new school said students city church mexico also schools university\n"
     ]
    }
   ],
   "source": [
    "# Print top 25 words of each topic\n",
    "print(\"Top 25 words by topic\")\n",
    "for k in range(n_topics):\n",
    "    print('#{}: {}'.format(k, ' '.join([w for (w, prop) in mdl.get_topic_words(k, top_n=10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958819dd-761a-495d-9e6a-d236722fac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.7)\n",
      "| 597652 docs, 145742375 words\n",
      "| Total Vocabs: 429180, Used Vocabs: 429180\n",
      "| Entropy of words: 8.87935\n",
      "| Entropy of term-weighted words: 8.87935\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 1000, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -9.12921\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 0 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 10 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 3315092819 (random seed)\n",
      "| trained in version 0.12.7\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.03475513 0.1140727  0.1753581  0.12145794 0.10705247 0.07604268\n",
      "|   0.09129351 0.07703894 0.12590477 0.07950267]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (7237333) : first game last two team\n",
      "| #1 (15717096) : would tax health new percent\n",
      "| #2 (21067870) : like one people get think\n",
      "| #3 (21136942) : us one people even would\n",
      "| #4 (13086474) : said police court two state\n",
      "| #5 (9895872) : said new water energy also\n",
      "| #6 (17403787) : company said percent stock market\n",
      "| #7 (15100372) : said military government united president\n",
      "| #8 (16654505) : trump president said would house\n",
      "| #9 (8442124) : new school said students city\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Survey stats. Notably, I'm using all the data as training data - you can also\n",
    "# generate metrics using the mdl.infer() method if you want to use held-out\n",
    "# data to see if the topic model generalizes well.\n",
    "# See https://bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel.infer\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181abe0f-d8d5-4e94-9818-0010b7e65b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
