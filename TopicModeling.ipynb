{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42ab321b-cb2a-4b8b-b4a9-e90b91209bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: tomotopy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.12.7)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tomotopy) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18681b29-a470-44af-9ae1-2e04f21ba269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c02d67-0ced-4f68-8bdb-26a411175667",
   "metadata": {},
   "source": [
    "## Access XML data for topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fb2ae85-8de6-49f6-ad43-b1ce4aeeb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f0b1705-94a8-4124-8bfa-b9b82f47e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tree = et.parse(\"data/articles-training-byarticle-20181122.xml\")\n",
    "publisher_tree = et.parse(\"data/articles-training-bypublisher-20181122.xml\")\n",
    "\n",
    "article_root = article_tree.getroot()\n",
    "publisher_root = publisher_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb5e6884-4fe2-4d49-94ad-36f8a17e70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fwright/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "11686f0e-fb1f-4b7a-8339-db735ca32e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'further', 'but', 'having', 'very', 't', 'or', 'isn', 'y', 'had', 'can', \"wasn't\", 'after', 'wouldn', 'before', 'itself', 'each', 'haven', 'do', \"hadn't\", 'yours', 'just', 'nor', 'and', 'will', 've', 'she', 'then', 's', 'both', 'him', 'there', 'how', 'ain', 'all', 'because', 'mightn', 'again', 'i', 'ours', 'over', 'we', 'off', 'you', 'being', 'weren', 'didn', 'which', 'no', 'this', 'on', 'only', 'he', 'same', \"don't\", 'with', 'hasn', 'at', \"shouldn't\", 'll', 'be', 'our', 'needn', 'ourselves', 'shouldn', 'aren', 'does', 'until', 'themselves', \"isn't\", 'so', \"didn't\", 'of', 'them', 'mustn', 'doing', 'to', \"aren't\", 'in', 'when', 'about', 'theirs', 'these', 'too', \"you're\", 'should', 'it', 'now', 'as', 'an', 'up', \"that'll\", 'more', 'your', 'few', 'that', 'ma', 'has', 'was', \"you've\", \"you'd\", 'm', \"weren't\", \"should've\", 'above', 'its', \"hasn't\", 'd', 'yourselves', \"she's\", 'herself', \"couldn't\", 'their', 'were', 'own', 'during', 'doesn', 'by', \"needn't\", 'am', 'some', 'any', 'into', 'most', 'hers', 'been', \"you'll\", 'his', 'her', 'under', 'such', 'himself', 'did', 'what', 'myself', 'if', 'against', \"doesn't\", 'where', 'whom', 'from', 'have', 'out', 'other', 'while', 'who', 'the', 'won', \"shan't\", \"wouldn't\", \"haven't\", 'my', 'me', 'why', \"won't\", 'down', 'through', 'are', 'than', 'is', 'for', 'below', 'o', 'once', \"it's\", 'shan', 'hadn', 'wasn', 'between', 'they', 're', 'not', 'here', \"mustn't\", 'don', 'a', \"mightn't\", 'yourself', 'those', 'couldn'}\n"
     ]
    }
   ],
   "source": [
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66769e5-48a0-4bd2-826f-f3c0f15db599",
   "metadata": {},
   "source": [
    "In the cell below, we get the text out of each of our articles to pass into the topic model. We apply lowercase to all of our words so that words with capital letters are treated the same as their lowercase counterparts, and so that words with capital letters will also be checked against the stoplist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91ba3109-49eb-4239-a653-ebb370fa3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "\n",
    "for article in article_root:\n",
    "    article_text = []\n",
    "    for paragraph in article:\n",
    "        if paragraph.text is not None:\n",
    "            article_text += [word for word in paragraph.text.lower().split() if word not in stoplist and word.isalpha()]\n",
    "    article_list += [article_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45e54b-6acb-43fe-b573-ad5b8cbc2ed3",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0e2a771-3746-4e8b-a317-085057434095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/fwright/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/fwright/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# movie reviews dataset, TODO: remove\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import movie_reviews\n",
    "file_ids = movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01d967be-1dda-426c-bb37-feb1573233fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-topic model populated with documents from our reviews (with stopwords removed)\n",
    "n_topics = 10\n",
    "n_docs = len(article_list)\n",
    "\n",
    "mdl = tomotopy.LDAModel(k=n_topics)\n",
    "for article in article_list:\n",
    "  mdl.add_doc(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dcba743a-1818-4fdb-acb8-6b237588fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50\tLog-likelihood: -9.032468042409862\n",
      "Iteration: 100\tLog-likelihood: -8.948392593473928\n",
      "Iteration: 150\tLog-likelihood: -8.906582864824466\n",
      "Iteration: 200\tLog-likelihood: -8.899380021034613\n",
      "Iteration: 250\tLog-likelihood: -8.888851858546053\n",
      "Iteration: 300\tLog-likelihood: -8.872216646944098\n",
      "Iteration: 350\tLog-likelihood: -8.86752147324241\n",
      "Iteration: 400\tLog-likelihood: -8.859647285779737\n",
      "Iteration: 450\tLog-likelihood: -8.862539261893579\n",
      "Iteration: 500\tLog-likelihood: -8.861383730908267\n",
      "Iteration: 550\tLog-likelihood: -8.856491308112238\n",
      "Iteration: 600\tLog-likelihood: -8.855941623347977\n",
      "Iteration: 650\tLog-likelihood: -8.852751379981433\n",
      "Iteration: 700\tLog-likelihood: -8.853367578415355\n",
      "Iteration: 750\tLog-likelihood: -8.85422254582576\n",
      "Iteration: 800\tLog-likelihood: -8.844701276642022\n",
      "Iteration: 850\tLog-likelihood: -8.84463768760105\n",
      "Iteration: 900\tLog-likelihood: -8.845150533087159\n",
      "Iteration: 950\tLog-likelihood: -8.841082747211596\n",
      "Iteration: 1000\tLog-likelihood: -8.849710062506954\n"
     ]
    }
   ],
   "source": [
    "# Most converging will happen fast, but we'll run for 1000 iterations just in case\n",
    "# (this will take a minute)\n",
    "iters_per_check = 50\n",
    "for i in range(0, 1000, iters_per_check):\n",
    "    mdl.train(iters_per_check)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i+iters_per_check, mdl.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5336d5af-f210-453e-bfb0-469cbe104d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words by topic\n",
      "#0: hurricane global florida warming still gold also climate data natural\n",
      "#1: people like think one know going get time want say\n",
      "#2: one would us even also media government political many article\n",
      "#3: money illegal law federal country must million immigration border state\n",
      "#4: trump president said donald clinton obama hillary white new house\n",
      "#5: police said people daily free man officers message according las\n",
      "#6: white black america left people american class liberal violence world\n",
      "#7: california los nfl players national israel anthem many stand protest\n",
      "#8: clinton fbi news investigation department former said comey hillary emails\n",
      "#9: north press attack war military united news muslim korea august\n"
     ]
    }
   ],
   "source": [
    "# Print top 25 words of each topic\n",
    "print(\"Top 25 words by topic\")\n",
    "for k in range(n_topics):\n",
    "    print('#{}: {}'.format(k, ' '.join([w for (w, prop) in mdl.get_topic_words(k, top_n=10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958819dd-761a-495d-9e6a-d236722fac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.7)\n",
      "| 2000 docs, 701543 words\n",
      "| Total Vocabs: 38738, Used Vocabs: 38738\n",
      "| Entropy of words: 8.65166\n",
      "| Entropy of term-weighted words: 8.65166\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 1000, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -9.09719\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 0 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 10 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 1247601995 (random seed)\n",
      "| trained in version 0.12.7\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.5152482  0.4451016  0.42984024 3.1232266  0.41613084 0.4096998\n",
      "|   4.1794796  0.6348134  1.2351668  0.4852954 ]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (30782) : sex school high kevin comedy\n",
      "| #1 (28643) : disney family kids little old\n",
      "| #2 (34511) : effects star special alien earth\n",
      "| #3 (176084) : film one story character characters\n",
      "| #4 (26274) : horror killer scream batman truman\n",
      "| #5 (26797) : action jackie fight cop van\n",
      "| #6 (239161) : movie one like film good\n",
      "| #7 (36920) : funny comedy jokes big smith\n",
      "| #8 (71784) : life love man world story\n",
      "| #9 (30587) : war men american black joe\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Survey stats. Notably, I'm using all the data as training data - you can also\n",
    "# generate metrics using the mdl.infer() method if you want to use held-out\n",
    "# data to see if the topic model generalizes well.\n",
    "# See https://bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel.infer\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181abe0f-d8d5-4e94-9818-0010b7e65b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
