{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ab321b-cb2a-4b8b-b4a9-e90b91209bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: tomotopy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.12.7)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tomotopy) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18681b29-a470-44af-9ae1-2e04f21ba269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c02d67-0ced-4f68-8bdb-26a411175667",
   "metadata": {},
   "source": [
    "## Access XML data for topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb2ae85-8de6-49f6-ad43-b1ce4aeeb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0b1705-94a8-4124-8bfa-b9b82f47e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_training_tree = et.parse(\"data/articles-training-byarticle-20181122.xml\")\n",
    "publisher_training_tree = et.parse(\"data/articles-training-bypublisher-20181122.xml\")\n",
    "publisher_validation_tree = et.parse(\"data/articles-validation-bypublisher-20181122.xml\")\n",
    "\n",
    "article_training_root = article_training_tree.getroot()\n",
    "publisher_training_root = publisher_training_tree.getroot()\n",
    "publisher_validation_root = publisher_validation_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5e6884-4fe2-4d49-94ad-36f8a17e70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fwright/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11686f0e-fb1f-4b7a-8339-db735ca32e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'herself', 'having', 'too', 'should', 'doing', 'about', 'weren', 'ain', 'off', 'him', 'we', 'and', \"you'd\", 'itself', 'ours', 'under', 'between', 'have', 'to', \"it's\", 'during', 'how', \"mightn't\", 'be', 'again', 'yourselves', 'myself', 'which', 'she', 'all', 'do', 'no', 'his', 'had', 'some', 'below', 'only', 'same', \"aren't\", 'where', 'before', 'nor', 'who', 'if', 'until', 'for', 'what', 'both', 'by', 'above', 'himself', 'other', 'once', 'over', \"she's\", \"you'll\", 'or', 'whom', 'while', \"weren't\", 'my', 'her', 'but', 'on', 'don', 'here', \"needn't\", 'this', \"shouldn't\", 'i', 'hers', 'an', 'does', 'own', 'just', 've', 'why', 'with', \"don't\", 'me', \"won't\", 'few', 'been', 'wasn', 'doesn', 'very', 'your', 'yourself', 'll', 'then', 'in', 'the', 'will', 'aren', 'hadn', 'being', 'shan', 'after', 'down', \"haven't\", 'a', 'am', 'so', 'hasn', \"you're\", 'shouldn', 'them', 'its', 'is', 'from', 'more', 'that', 'can', \"should've\", 'most', 'mightn', 'ourselves', 'there', 'when', \"wouldn't\", 'are', 'haven', 'isn', 'into', \"that'll\", 'up', \"didn't\", 'our', 'yours', 'such', 're', 'mustn', 't', 'out', 'themselves', \"hasn't\", 'wouldn', \"isn't\", 'than', 's', \"you've\", 'o', \"couldn't\", 'they', 'you', \"wasn't\", 'not', 'these', \"hadn't\", 'd', 'because', 'couldn', 'needn', 'now', 'has', 'further', 'of', 'didn', 'through', 'ma', 'any', 'were', 'it', 'each', 'as', 'he', 'at', 'theirs', 'did', 'was', \"mustn't\", \"shan't\", 'those', 'against', 'm', 'y', \"doesn't\", 'their', 'won'}\n"
     ]
    }
   ],
   "source": [
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66769e5-48a0-4bd2-826f-f3c0f15db599",
   "metadata": {},
   "source": [
    "In the cell below, we get the text out of each of our articles to pass into the topic model. We apply lowercase to all of our words so that words with capital letters are treated the same as their lowercase counterparts, and so that words with capital letters will also be checked against the stoplist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ba3109-49eb-4239-a653-ebb370fa3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n"
     ]
    }
   ],
   "source": [
    "article_list = []\n",
    "\n",
    "for article in article_training_root:\n",
    "    article_text = []\n",
    "    for paragraph in article:\n",
    "        if paragraph.text is not None:\n",
    "            article_text += [word for word in paragraph.text.lower().split() if word not in stoplist and word.isalpha()]\n",
    "    article_list += [article_text]\n",
    "print(len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75dbabbd-d150-4115-99a3-c1f1577d546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600645\n"
     ]
    }
   ],
   "source": [
    "for article in publisher_training_root:\n",
    "    article_text = []\n",
    "    for paragraph in article:\n",
    "        if paragraph.text is not None:\n",
    "            article_text += [word for word in paragraph.text.lower().split() if word not in stoplist and word.isalpha()]\n",
    "    article_list += [article_text]\n",
    "print(len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64123b9a-c69b-46ff-b8d8-73a248e74fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750645\n"
     ]
    }
   ],
   "source": [
    "for article in publisher_validation_root:\n",
    "    article_text = []\n",
    "    for paragraph in article:\n",
    "        if paragraph.text is not None:\n",
    "            article_text += [word for word in paragraph.text.lower().split() if word not in stoplist and word.isalpha()]\n",
    "    article_list += [article_text]\n",
    "print(len(article_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45e54b-6acb-43fe-b573-ad5b8cbc2ed3",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d967be-1dda-426c-bb37-feb1573233fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_docs = len(article_list)\n",
    "\n",
    "mdl = tomotopy.LDAModel(k=n_topics)\n",
    "for article in article_list:\n",
    "  mdl.add_doc(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcba743a-1818-4fdb-acb8-6b237588fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50\tLog-likelihood: -9.191219393310083\n",
      "Iteration: 100\tLog-likelihood: -9.165878648728706\n",
      "Iteration: 150\tLog-likelihood: -9.15899475218593\n",
      "Iteration: 200\tLog-likelihood: -9.15526140210349\n",
      "Iteration: 250\tLog-likelihood: -9.153324367351884\n",
      "Iteration: 300\tLog-likelihood: -9.152337779219758\n",
      "Iteration: 350\tLog-likelihood: -9.15157365945748\n",
      "Iteration: 400\tLog-likelihood: -9.151642561490215\n",
      "Iteration: 450\tLog-likelihood: -9.15105437756308\n",
      "Iteration: 500\tLog-likelihood: -9.150963935020132\n",
      "Iteration: 550\tLog-likelihood: -9.151049332226332\n",
      "Iteration: 600\tLog-likelihood: -9.150794449728673\n",
      "Iteration: 650\tLog-likelihood: -9.150846074384429\n",
      "Iteration: 700\tLog-likelihood: -9.150788928092108\n",
      "Iteration: 750\tLog-likelihood: -9.150897518997581\n",
      "Iteration: 800\tLog-likelihood: -9.15086211854652\n",
      "Iteration: 850\tLog-likelihood: -9.150927204039723\n",
      "Iteration: 900\tLog-likelihood: -9.150922063091924\n",
      "Iteration: 950\tLog-likelihood: -9.151005327765473\n",
      "Iteration: 1000\tLog-likelihood: -9.150957556647533\n"
     ]
    }
   ],
   "source": [
    "iters_per_check = 50\n",
    "for i in range(0, 1000, iters_per_check):\n",
    "    mdl.train(iters_per_check)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i+iters_per_check, mdl.ll_per_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5336d5af-f210-453e-bfb0-469cbe104d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words by topic\n",
      "#0: tax would percent economic money financial government health pay could\n",
      "#1: trump president said obama republican would house campaign white republicans\n",
      "#2: new one like first said also people says two time\n",
      "#3: first game two last team points said second one coach\n",
      "#4: said new water energy climate could also one oil people\n",
      "#5: said police told two according one say people man also\n",
      "#6: united military government president war would said states us international\n",
      "#7: people one like would even think us many get know\n",
      "#8: state said new would school public law court federal also\n",
      "#9: company said percent stock new continue million last sales market\n"
     ]
    }
   ],
   "source": [
    "# Print top 25 words of each topic\n",
    "print(\"Top 25 words by topic\")\n",
    "for k in range(n_topics):\n",
    "    print('#{}: {}'.format(k, ' '.join([w for (w, prop) in mdl.get_topic_words(k, top_n=10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "958819dd-761a-495d-9e6a-d236722fac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.7)\n",
      "| 746557 docs, 187266574 words\n",
      "| Total Vocabs: 477650, Used Vocabs: 477650\n",
      "| Entropy of words: 8.86260\n",
      "| Entropy of term-weighted words: 8.86260\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 1000, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -9.15096\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 0 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 10 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 2643793627 (random seed)\n",
      "| trained in version 0.12.7\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.08584844 0.1295086  0.12636134 0.03294846 0.07857875 0.10443414\n",
      "|   0.0899034  0.20284444 0.11802528 0.07578992]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (17074208) : tax would percent economic money\n",
      "| #1 (20138110) : trump president said obama republican\n",
      "| #2 (18237788) : new one like first said\n",
      "| #3 (7820499) : first game two last team\n",
      "| #4 (13025157) : said new water energy climate\n",
      "| #5 (14603146) : said police told two according\n",
      "| #6 (25374598) : united military government president war\n",
      "| #7 (35722326) : people one like would even\n",
      "| #8 (18412437) : state said new would school\n",
      "| #9 (16858305) : company said percent stock new\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Survey stats. Notably, I'm using all the data as training data - you can also\n",
    "# generate metrics using the mdl.infer() method if you want to use held-out\n",
    "# data to see if the topic model generalizes well.\n",
    "# See https://bab2min.github.io/tomotopy/v0.4.1/en/#tomotopy.LDAModel.infer\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181abe0f-d8d5-4e94-9818-0010b7e65b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
